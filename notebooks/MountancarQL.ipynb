{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977a3fe6-b8cd-4e5c-b661-9e42087cbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils import utils\n",
    "from src.utils.kbins_discretizator import KBinsDiscretizator\n",
    "from src.agents.q_agent import QAgent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b87ed6-8d76-47c2-816c-d56386c7e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 1500\n",
    "N_STEPS = 1000\n",
    "\n",
    "EXPLORATION_RATIO = 0.8\n",
    "LEARNING_RATE = 0.9\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "E_DECAY_LIMIT = 0.05\n",
    "E_DECAY_RATE = 0.01\n",
    "\n",
    "BINS_POS = 100\n",
    "BINS_VEL = 100\n",
    "\n",
    "RENDER = False\n",
    "REPORT_FILE = False\n",
    "\n",
    "config = {\n",
    "    \"n_episodes\": N_EPISODES,\n",
    "    \"n_steps\": N_STEPS,\n",
    "    \"exploration_ratio\": EXPLORATION_RATIO,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"discount_factor\": DISCOUNT_FACTOR,\n",
    "    \"e_decay_limit\": E_DECAY_LIMIT,\n",
    "    \"e_decay_rate\": E_DECAY_RATE,\n",
    "    \"bin_pos\": BINS_POS,\n",
    "    \"bin_vel\": BINS_VEL,\n",
    "    \"render\": RENDER,\n",
    "    \"report_file\": REPORT_FILE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e0b791-3ad6-407b-9d16-ecb7e5658a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = N_STEPS\n",
    "\n",
    "# TODO: Tengo que ver cuales son los nombres de las acciones\n",
    "actions_dict = {0: 'Zero', 1: 'One', 2: 'Two'}\n",
    "hist = {}\n",
    "\n",
    "discretizator = KBinsDiscretizator(env.observation_space.low, env.observation_space.high, bins_array=[BINS_POS, BINS_VEL], encode='ordinal', strategy='uniform')\n",
    "\n",
    "agent = QAgent(discretizator.get_n_states(), env.action_space, exploration_ratio=EXPLORATION_RATIO,\n",
    "               learning_rate=LEARNING_RATE, discount_factor=DISCOUNT_FACTOR, e_decay_limit=E_DECAY_LIMIT, e_decay_rate=E_DECAY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9db2152-fac6-484c-8d35-92e33573bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############### Ini Training ###############\n",
      "\n",
      "Episode: 0\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.79\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 10\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.69\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 20\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.59\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 30\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.49\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 40\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.39\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 50\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.29\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 60\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.19\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 70\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.09\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 80\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 90\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 100\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 110\t\tReward: -589.0\t\tSteps: 589\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 120\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 130\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 140\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': False}\n",
      "Episode: 150\t\tReward: -758.0\t\tSteps: 758\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 160\t\tReward: -654.0\t\tSteps: 654\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 170\t\tReward: -850.0\t\tSteps: 850\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 180\t\tReward: -584.0\t\tSteps: 584\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 190\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 200\t\tReward: -693.0\t\tSteps: 693\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 210\t\tReward: -1000.0\t\tSteps: 1000\t\tEpsilon: 0.05\t\tInfo: {'TimeLimit.truncated': True}\n",
      "Episode: 220\t\tReward: -872.0\t\tSteps: 872\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 230\t\tReward: -793.0\t\tSteps: 793\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 240\t\tReward: -648.0\t\tSteps: 648\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 250\t\tReward: -465.0\t\tSteps: 465\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 260\t\tReward: -835.0\t\tSteps: 835\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 270\t\tReward: -380.0\t\tSteps: 380\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 280\t\tReward: -490.0\t\tSteps: 490\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 290\t\tReward: -481.0\t\tSteps: 481\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 300\t\tReward: -370.0\t\tSteps: 370\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 310\t\tReward: -798.0\t\tSteps: 798\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 320\t\tReward: -453.0\t\tSteps: 453\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 330\t\tReward: -603.0\t\tSteps: 603\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 340\t\tReward: -530.0\t\tSteps: 530\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 350\t\tReward: -321.0\t\tSteps: 321\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 360\t\tReward: -464.0\t\tSteps: 464\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 370\t\tReward: -499.0\t\tSteps: 499\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 380\t\tReward: -329.0\t\tSteps: 329\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 390\t\tReward: -548.0\t\tSteps: 548\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 400\t\tReward: -488.0\t\tSteps: 488\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 410\t\tReward: -334.0\t\tSteps: 334\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 420\t\tReward: -409.0\t\tSteps: 409\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 430\t\tReward: -503.0\t\tSteps: 503\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 440\t\tReward: -401.0\t\tSteps: 401\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 450\t\tReward: -409.0\t\tSteps: 409\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 460\t\tReward: -311.0\t\tSteps: 311\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 470\t\tReward: -268.0\t\tSteps: 268\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 480\t\tReward: -490.0\t\tSteps: 490\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 490\t\tReward: -343.0\t\tSteps: 343\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 500\t\tReward: -871.0\t\tSteps: 871\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 510\t\tReward: -536.0\t\tSteps: 536\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 520\t\tReward: -409.0\t\tSteps: 409\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 530\t\tReward: -414.0\t\tSteps: 414\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 540\t\tReward: -364.0\t\tSteps: 364\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 550\t\tReward: -362.0\t\tSteps: 362\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 560\t\tReward: -444.0\t\tSteps: 444\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 570\t\tReward: -329.0\t\tSteps: 329\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 580\t\tReward: -439.0\t\tSteps: 439\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 590\t\tReward: -446.0\t\tSteps: 446\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 600\t\tReward: -679.0\t\tSteps: 679\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 610\t\tReward: -679.0\t\tSteps: 679\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 620\t\tReward: -384.0\t\tSteps: 384\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 630\t\tReward: -392.0\t\tSteps: 392\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 640\t\tReward: -443.0\t\tSteps: 443\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 650\t\tReward: -577.0\t\tSteps: 577\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 660\t\tReward: -409.0\t\tSteps: 409\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 670\t\tReward: -462.0\t\tSteps: 462\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 680\t\tReward: -571.0\t\tSteps: 571\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 690\t\tReward: -362.0\t\tSteps: 362\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 700\t\tReward: -412.0\t\tSteps: 412\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 710\t\tReward: -587.0\t\tSteps: 587\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 720\t\tReward: -397.0\t\tSteps: 397\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 730\t\tReward: -547.0\t\tSteps: 547\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 740\t\tReward: -329.0\t\tSteps: 329\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 750\t\tReward: -504.0\t\tSteps: 504\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 760\t\tReward: -446.0\t\tSteps: 446\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 770\t\tReward: -276.0\t\tSteps: 276\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 780\t\tReward: -329.0\t\tSteps: 329\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 790\t\tReward: -443.0\t\tSteps: 443\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 800\t\tReward: -422.0\t\tSteps: 422\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 810\t\tReward: -317.0\t\tSteps: 317\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 820\t\tReward: -240.0\t\tSteps: 240\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 830\t\tReward: -313.0\t\tSteps: 313\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 840\t\tReward: -482.0\t\tSteps: 482\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 850\t\tReward: -357.0\t\tSteps: 357\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 860\t\tReward: -321.0\t\tSteps: 321\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 870\t\tReward: -321.0\t\tSteps: 321\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 880\t\tReward: -370.0\t\tSteps: 370\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 890\t\tReward: -342.0\t\tSteps: 342\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 900\t\tReward: -351.0\t\tSteps: 351\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 910\t\tReward: -321.0\t\tSteps: 321\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 920\t\tReward: -276.0\t\tSteps: 276\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 930\t\tReward: -329.0\t\tSteps: 329\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 940\t\tReward: -461.0\t\tSteps: 461\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 950\t\tReward: -446.0\t\tSteps: 446\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 960\t\tReward: -330.0\t\tSteps: 330\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 970\t\tReward: -439.0\t\tSteps: 439\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 980\t\tReward: -316.0\t\tSteps: 316\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 990\t\tReward: -283.0\t\tSteps: 283\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1000\t\tReward: -401.0\t\tSteps: 401\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1010\t\tReward: -293.0\t\tSteps: 293\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1020\t\tReward: -326.0\t\tSteps: 326\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1030\t\tReward: -327.0\t\tSteps: 327\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1040\t\tReward: -283.0\t\tSteps: 283\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1050\t\tReward: -482.0\t\tSteps: 482\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1060\t\tReward: -279.0\t\tSteps: 279\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1070\t\tReward: -295.0\t\tSteps: 295\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1080\t\tReward: -236.0\t\tSteps: 236\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1090\t\tReward: -381.0\t\tSteps: 381\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1100\t\tReward: -247.0\t\tSteps: 247\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1110\t\tReward: -282.0\t\tSteps: 282\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1120\t\tReward: -334.0\t\tSteps: 334\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1130\t\tReward: -410.0\t\tSteps: 410\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1140\t\tReward: -422.0\t\tSteps: 422\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1150\t\tReward: -410.0\t\tSteps: 410\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1160\t\tReward: -269.0\t\tSteps: 269\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1170\t\tReward: -259.0\t\tSteps: 259\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1180\t\tReward: -276.0\t\tSteps: 276\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1190\t\tReward: -312.0\t\tSteps: 312\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1200\t\tReward: -330.0\t\tSteps: 330\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1210\t\tReward: -281.0\t\tSteps: 281\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1220\t\tReward: -421.0\t\tSteps: 421\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1230\t\tReward: -326.0\t\tSteps: 326\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1240\t\tReward: -291.0\t\tSteps: 291\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1250\t\tReward: -297.0\t\tSteps: 297\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1260\t\tReward: -197.0\t\tSteps: 197\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1270\t\tReward: -374.0\t\tSteps: 374\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1280\t\tReward: -245.0\t\tSteps: 245\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1290\t\tReward: -246.0\t\tSteps: 246\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1300\t\tReward: -506.0\t\tSteps: 506\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1310\t\tReward: -209.0\t\tSteps: 209\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1320\t\tReward: -226.0\t\tSteps: 226\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1330\t\tReward: -325.0\t\tSteps: 325\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1340\t\tReward: -253.0\t\tSteps: 253\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1350\t\tReward: -240.0\t\tSteps: 240\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1360\t\tReward: -225.0\t\tSteps: 225\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1370\t\tReward: -288.0\t\tSteps: 288\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1380\t\tReward: -285.0\t\tSteps: 285\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1390\t\tReward: -172.0\t\tSteps: 172\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1400\t\tReward: -326.0\t\tSteps: 326\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1410\t\tReward: -277.0\t\tSteps: 277\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1420\t\tReward: -445.0\t\tSteps: 445\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1430\t\tReward: -291.0\t\tSteps: 291\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1440\t\tReward: -196.0\t\tSteps: 196\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1450\t\tReward: -264.0\t\tSteps: 264\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1460\t\tReward: -396.0\t\tSteps: 396\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1470\t\tReward: -279.0\t\tSteps: 279\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1480\t\tReward: -319.0\t\tSteps: 319\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "Episode: 1490\t\tReward: -242.0\t\tSteps: 242\t\tEpsilon: 0.05\t\tInfo: {}\n",
      "\n",
      "############### End Training ###############\n",
      "\n",
      "\n",
      "\n",
      "################## Report ##################\n",
      "\n",
      "Average reward: -460.80266666666665\n",
      "Average reward of last 10%(150): -301.26\n",
      "Average steps: 460.80266666666665\n",
      "Average steps of last 10%(150): 301.26\n",
      "\n",
      "Q-table:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "################ End Report ################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n############### Ini Training ###############\\n\")\n",
    "for i_episode in range(N_EPISODES):\n",
    "    state = env.reset()\n",
    "    reward_counter = 0\n",
    "    if RENDER:\n",
    "        print(\"############### Ini Episode\", i_episode, \"###############\")\n",
    "    for t in range(N_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "            print(\"Actual State:\", state)\n",
    "        action = agent.get_next_step(discretizator.idx_state(state))\n",
    "        if RENDER:\n",
    "            print(\"Action:\", actions_dict[action])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward_counter += reward\n",
    "        if RENDER:\n",
    "            print(\"Next State:\", next_state, \"\\n\")\n",
    "        agent.update_qtable(discretizator.idx_state(state), action, reward, discretizator.idx_state(next_state), done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    agent.greedy_decay()\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode: {}\\t\\tReward: {}\\t\\tSteps: {}\\t\\tEpsilon: {:.2f}\\t\\tInfo: {}'.format(i_episode, reward_counter, t+1, agent.exploration_ratio, info))\n",
    "    hist[i_episode] = {'reward': reward_counter, 'steps': t+1}\n",
    "    if RENDER:\n",
    "        print(\"############### End Episode\", i_episode, \"###############\")\n",
    "print(\"\\n############### End Training ###############\\n\")\n",
    "print(\"\\n\\n################## Report ##################\\n\")\n",
    "report = {\"average_reward\": utils.get_average_reward_last_n(hist, N_EPISODES),\n",
    "            \"average_reward_last_10\": utils.get_average_reward_last_n(hist, int(N_EPISODES*0.1)),\n",
    "            \"average_steps\": utils.get_average_steps_last_n(hist, N_EPISODES),\n",
    "            \"average_steps_last_10\": utils.get_average_steps_last_n(hist, int(N_EPISODES*0.1))\n",
    "            }\n",
    "print(\"Average reward:\", report[\"average_reward\"])\n",
    "print(\"Average reward of last 10%(\"+str(int(N_EPISODES*0.1))+\"):\",report[\"average_reward_last_10\"])\n",
    "print(\"Average steps:\", report[\"average_steps\"])\n",
    "print(\"Average steps of last 10%(\"+str(int(N_EPISODES*0.1))+\"):\",report[\"average_steps_last_10\"])\n",
    "print(\"\\nQ-table:\")\n",
    "print(agent.qtable)\n",
    "print(\"\\n################ End Report ################\")\n",
    "if REPORT_FILE:\n",
    "    utils.generate_report_file(config, report, hist, agent.qtable)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c91a57-4bf8-4802-966c-b33d95120b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clever_agent = agent = QAgent(discretizator.get_n_states(), env.action_space, qtable=agent.get_qtable(), exploration_ratio=0,\n",
    "               learning_rate=0, discount_factor=0, e_decay_limit=0, e_decay_rate=0)\n",
    "\n",
    "env_to_wrap = gym.make('MountainCar-v0')\n",
    "env_to_wrap._max_episode_steps = N_STEPS\n",
    "env = wrappers.Monitor(env_to_wrap, \"./resources/videos\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = clever_agent.get_next_step(discretizator.idx_state(state))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: break\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0e168-65fc-4f35-970d-3f196c7a3f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
